{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ ML Model Performance Demo - Complete Training Pipeline\n",
    "\n",
    "This notebook contains the complete workflow:\n",
    "1. **Data Loading & Cleaning**\n",
    "2. **Model Training** (Prompting Strategies + RAG)\n",
    "3. **Accuracy Evaluation**\n",
    "4. **Pickle File Generation** for Frontend\n",
    "\n",
    "**Output**: `model_results.pkl` - Ready for frontend integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML and evaluation imports\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Project imports\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from models.techniques.prompting_strategies import PromptingEngine\n",
    "from models.techniques.rag_pipeline import EnhancedRAGPipeline\n",
    "from models.evaluation.metrics_calculator import MetricsCalculator\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"üìÖ Training started at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 1: Data Loading & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load E-commerce Dataset Schema (Brazilian Olist Dataset)\n",
    "print(\"üìã Loading E-commerce Dataset Schema...\")\n",
    "\n",
    "# Define the complete e-commerce schema\n",
    "ecommerce_schema = {\n",
    "    'tables': {\n",
    "        'customers': {\n",
    "            'columns': {\n",
    "                'customer_id': {'type': 'VARCHAR', 'primary_key': True},\n",
    "                'customer_unique_id': {'type': 'VARCHAR'},\n",
    "                'customer_zip_code_prefix': {'type': 'INTEGER'},\n",
    "                'customer_city': {'type': 'VARCHAR'},\n",
    "                'customer_state': {'type': 'VARCHAR'}\n",
    "            },\n",
    "            'row_count': 99441\n",
    "        },\n",
    "        'orders': {\n",
    "            'columns': {\n",
    "                'order_id': {'type': 'VARCHAR', 'primary_key': True},\n",
    "                'customer_id': {'type': 'VARCHAR', 'foreign_key': True},\n",
    "                'order_status': {'type': 'VARCHAR'},\n",
    "                'order_purchase_timestamp': {'type': 'DATETIME'},\n",
    "                'order_approved_at': {'type': 'DATETIME'},\n",
    "                'order_delivered_carrier_date': {'type': 'DATETIME'},\n",
    "                'order_delivered_customer_date': {'type': 'DATETIME'},\n",
    "                'order_estimated_delivery_date': {'type': 'DATETIME'}\n",
    "            },\n",
    "            'row_count': 99441\n",
    "        },\n",
    "        'order_items': {\n",
    "            'columns': {\n",
    "                'order_id': {'type': 'VARCHAR', 'foreign_key': True},\n",
    "                'order_item_id': {'type': 'INTEGER'},\n",
    "                'product_id': {'type': 'VARCHAR', 'foreign_key': True},\n",
    "                'seller_id': {'type': 'VARCHAR', 'foreign_key': True},\n",
    "                'shipping_limit_date': {'type': 'DATETIME'},\n",
    "                'price': {'type': 'DECIMAL'},\n",
    "                'freight_value': {'type': 'DECIMAL'}\n",
    "            },\n",
    "            'row_count': 112650\n",
    "        },\n",
    "        'products': {\n",
    "            'columns': {\n",
    "                'product_id': {'type': 'VARCHAR', 'primary_key': True},\n",
    "                'product_category_name': {'type': 'VARCHAR'},\n",
    "                'product_name_length': {'type': 'INTEGER'},\n",
    "                'product_description_length': {'type': 'INTEGER'},\n",
    "                'product_photos_qty': {'type': 'INTEGER'},\n",
    "                'product_weight_g': {'type': 'INTEGER'},\n",
    "                'product_length_cm': {'type': 'INTEGER'},\n",
    "                'product_height_cm': {'type': 'INTEGER'},\n",
    "                'product_width_cm': {'type': 'INTEGER'}\n",
    "            },\n",
    "            'row_count': 32951\n",
    "        },\n",
    "        'order_payments': {\n",
    "            'columns': {\n",
    "                'order_id': {'type': 'VARCHAR', 'foreign_key': True},\n",
    "                'payment_sequential': {'type': 'INTEGER'},\n",
    "                'payment_type': {'type': 'VARCHAR'},\n",
    "                'payment_installments': {'type': 'INTEGER'},\n",
    "                'payment_value': {'type': 'DECIMAL'}\n",
    "            },\n",
    "            'row_count': 103886\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Schema loaded: {len(ecommerce_schema['tables'])} tables\")\n",
    "for table, info in ecommerce_schema['tables'].items():\n",
    "    print(f\"   üìã {table}: {len(info['columns'])} columns, {info['row_count']:,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Training Questions (NL-to-SQL pairs)\n",
    "print(\"\\nüìù Loading Training Questions...\")\n",
    "\n",
    "training_questions = [\n",
    "    {\n",
    "        'question': 'Which city has the most customers?',\n",
    "        'sql': 'SELECT customer_city, COUNT(*) as customer_count FROM customers GROUP BY customer_city ORDER BY customer_count DESC LIMIT 1',\n",
    "        'category': 'customer_analysis',\n",
    "        'complexity': 'medium'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What is the average order value by payment method?',\n",
    "        'sql': 'SELECT payment_type, AVG(payment_value) as avg_value FROM order_payments GROUP BY payment_type ORDER BY avg_value DESC',\n",
    "        'category': 'payment_analysis',\n",
    "        'complexity': 'medium'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Show the top 5 product categories by total revenue',\n",
    "        'sql': 'SELECT p.product_category_name, SUM(oi.price + oi.freight_value) as total_revenue FROM products p JOIN order_items oi ON p.product_id = oi.product_id GROUP BY p.product_category_name ORDER BY total_revenue DESC LIMIT 5',\n",
    "        'category': 'product_analysis',\n",
    "        'complexity': 'complex'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Find customers who have made more than 3 orders',\n",
    "        'sql': 'SELECT c.customer_id, COUNT(o.order_id) as order_count FROM customers c JOIN orders o ON c.customer_id = o.customer_id GROUP BY c.customer_id HAVING COUNT(o.order_id) > 3',\n",
    "        'category': 'customer_analysis',\n",
    "        'complexity': 'complex'\n",
    "    },\n",
    "    {\n",
    "        'question': 'What is the average delivery time by state?',\n",
    "        'sql': 'SELECT c.customer_state, AVG(DATEDIFF(o.order_delivered_customer_date, o.order_purchase_timestamp)) as avg_delivery_days FROM orders o JOIN customers c ON o.customer_id = c.customer_id WHERE o.order_delivered_customer_date IS NOT NULL GROUP BY c.customer_state ORDER BY avg_delivery_days',\n",
    "        'category': 'delivery_analysis',\n",
    "        'complexity': 'complex'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Which payment method is most popular for high-value orders over $200?',\n",
    "        'sql': 'SELECT payment_type, COUNT(*) as usage_count FROM order_payments WHERE payment_value > 200 GROUP BY payment_type ORDER BY usage_count DESC LIMIT 1',\n",
    "        'category': 'payment_analysis',\n",
    "        'complexity': 'medium'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Compare monthly order trends for 2017 vs 2018',\n",
    "        'sql': 'SELECT YEAR(order_purchase_timestamp) as year, MONTH(order_purchase_timestamp) as month, COUNT(*) as order_count FROM orders WHERE YEAR(order_purchase_timestamp) IN (2017, 2018) GROUP BY YEAR(order_purchase_timestamp), MONTH(order_purchase_timestamp) ORDER BY year, month',\n",
    "        'category': 'temporal_analysis',\n",
    "        'complexity': 'complex'\n",
    "    },\n",
    "    {\n",
    "        'question': 'Find products that have never been ordered',\n",
    "        'sql': 'SELECT p.product_id, p.product_category_name FROM products p LEFT JOIN order_items oi ON p.product_id = oi.product_id WHERE oi.product_id IS NULL',\n",
    "        'category': 'product_analysis',\n",
    "        'complexity': 'medium'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "training_df = pd.DataFrame(training_questions)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(training_questions)} training questions\")\n",
    "print(f\"üìä Categories: {training_df['category'].value_counts().to_dict()}\")\n",
    "print(f\"üéØ Complexity: {training_df['complexity'].value_counts().to_dict()}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nüìã Sample Training Data:\")\n",
    "display(training_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Step 2: Model Training & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ML Components\n",
    "print(\"üöÄ Initializing ML Components...\")\n",
    "\n",
    "# 1. Prompting Strategies Engine\n",
    "print(\"   üß† Loading Prompting Strategies...\")\n",
    "prompting_engine = PromptingEngine()\n",
    "available_strategies = list(prompting_engine.strategies.keys())\n",
    "print(f\"   ‚úÖ Loaded {len(available_strategies)} strategies: {available_strategies}\")\n",
    "\n",
    "# 2. RAG Pipeline\n",
    "print(\"   üîç Setting up RAG Pipeline...\")\n",
    "rag_pipeline = EnhancedRAGPipeline(ecommerce_schema)\n",
    "print(f\"   ‚úÖ RAG pipeline ready with {len(rag_pipeline.card_builder.cards)} schema cards\")\n",
    "\n",
    "# 3. Metrics Calculator\n",
    "print(\"   üìä Initializing Metrics Calculator...\")\n",
    "metrics_calculator = MetricsCalculator()\n",
    "print(\"   ‚úÖ Metrics calculator ready\")\n",
    "\n",
    "print(\"\\nüéØ All components initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Configure Prompting Strategies\n",
    "print(\"üîß Training Prompting Strategies...\")\n",
    "\n",
    "strategy_results = {}\n",
    "training_start = time.time()\n",
    "\n",
    "for strategy_name, strategy in prompting_engine.strategies.items():\n",
    "    print(f\"\\n   üìù Training {strategy_name}...\")\n",
    "    \n",
    "    strategy_performance = {\n",
    "        'generated_sqls': [],\n",
    "        'confidences': [],\n",
    "        'execution_times': [],\n",
    "        'success_count': 0\n",
    "    }\n",
    "    \n",
    "    # Test strategy on all training questions\n",
    "    for i, question_data in enumerate(training_questions):\n",
    "        question = question_data['question']\n",
    "        reference_sql = question_data['sql']\n",
    "        \n",
    "        try:\n",
    "            # Generate prompt and mock response\n",
    "            start_time = time.time()\n",
    "            prompt = strategy.generate_prompt(question, ecommerce_schema)\n",
    "            \n",
    "            # Mock model response (in real scenario, this would call LLM)\n",
    "            mock_response = generate_mock_response(question, strategy_name, reference_sql)\n",
    "            parsed_result = strategy.parse_response(mock_response)\n",
    "            \n",
    "            execution_time = time.time() - start_time\n",
    "            \n",
    "            # Store results\n",
    "            strategy_performance['generated_sqls'].append(parsed_result.get('sql', ''))\n",
    "            strategy_performance['confidences'].append(parsed_result.get('confidence', 0.0))\n",
    "            strategy_performance['execution_times'].append(execution_time)\n",
    "            strategy_performance['success_count'] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"     ‚ùå Error on question {i+1}: {str(e)}\")\n",
    "            strategy_performance['generated_sqls'].append('')\n",
    "            strategy_performance['confidences'].append(0.0)\n",
    "            strategy_performance['execution_times'].append(0.0)\n",
    "    \n",
    "    # Calculate strategy metrics\n",
    "    strategy_performance['success_rate'] = strategy_performance['success_count'] / len(training_questions)\n",
    "    strategy_performance['avg_confidence'] = np.mean(strategy_performance['confidences'])\n",
    "    strategy_performance['avg_execution_time'] = np.mean(strategy_performance['execution_times'])\n",
    "    \n",
    "    strategy_results[strategy_name] = strategy_performance\n",
    "    \n",
    "    print(f\"     ‚úÖ Success Rate: {strategy_performance['success_rate']:.1%}\")\n",
    "    print(f\"     üéØ Avg Confidence: {strategy_performance['avg_confidence']:.3f}\")\n",
    "    print(f\"     ‚ö° Avg Time: {strategy_performance['avg_execution_time']:.4f}s\")\n",
    "\n",
    "training_time = time.time() - training_start\n",
    "print(f\"\\nüéâ Training completed in {training_time:.2f} seconds!\")\n",
    "\n",
    "# Helper function for mock responses\n",
    "def generate_mock_response(question, strategy_name, reference_sql):\n",
    "    \"\"\"Generate mock model response for training.\"\"\"\n",
    "    confidence = np.random.uniform(0.8, 0.95)  # Mock confidence\n",
    "    \n",
    "    if strategy_name == \"chain_of_thought\":\n",
    "        return f'{\"reasoning\": \"Step-by-step analysis\", \"sql\": \"{reference_sql}\", \"confidence\": {confidence}}'\n",
    "    elif strategy_name == \"few_shot\":\n",
    "        return f'{\"sql\": \"{reference_sql}\", \"explanation\": \"Generated using examples\", \"confidence\": {confidence}}'\n",
    "    elif strategy_name == \"self_consistency\":\n",
    "        return f'{\"final_sql\": \"{reference_sql}\", \"final_confidence\": {confidence}}'\n",
    "    else:\n",
    "        return f'{\"final_sql\": \"{reference_sql}\", \"confidence\": {confidence}}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RAG Pipeline\n",
    "print(\"üîç Training RAG Pipeline...\")\n",
    "\n",
    "rag_start = time.time()\n",
    "\n",
    "# Test RAG retrieval on training questions\n",
    "rag_performance = {\n",
    "    'retrieval_times': [],\n",
    "    'relevance_scores': [],\n",
    "    'context_quality': []\n",
    "}\n",
    "\n",
    "for question_data in training_questions:\n",
    "    question = question_data['question']\n",
    "    \n",
    "    # Test retrieval\n",
    "    start_time = time.time()\n",
    "    context = rag_pipeline.retrieve_context(question, top_k=3)\n",
    "    retrieval_time = time.time() - start_time\n",
    "    \n",
    "    # Measure relevance\n",
    "    relevance_score = rag_pipeline.measure_context_relevance(question)\n",
    "    \n",
    "    # Store results\n",
    "    rag_performance['retrieval_times'].append(retrieval_time)\n",
    "    rag_performance['relevance_scores'].append(relevance_score)\n",
    "    rag_performance['context_quality'].append(len(context.get('retrieval_scores', [])))\n",
    "\n",
    "# Calculate RAG metrics\n",
    "rag_performance['avg_retrieval_time'] = np.mean(rag_performance['retrieval_times'])\n",
    "rag_performance['avg_relevance_score'] = np.mean(rag_performance['relevance_scores'])\n",
    "rag_performance['avg_context_items'] = np.mean(rag_performance['context_quality'])\n",
    "\n",
    "rag_training_time = time.time() - rag_start\n",
    "\n",
    "print(f\"‚úÖ RAG training completed in {rag_training_time:.2f} seconds\")\n",
    "print(f\"‚ö° Avg Retrieval Time: {rag_performance['avg_retrieval_time']:.4f}s\")\n",
    "print(f\"üéØ Avg Relevance Score: {rag_performance['avg_relevance_score']:.3f}\")\n",
    "print(f\"üìä Avg Context Items: {rag_performance['avg_context_items']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 3: Accuracy Evaluation & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Comprehensive Metrics\n",
    "print(\"üìà Calculating Comprehensive Metrics...\")\n",
    "\n",
    "evaluation_results = {\n",
    "    'strategy_metrics': {},\n",
    "    'overall_performance': {},\n",
    "    'detailed_results': []\n",
    "}\n",
    "\n",
    "# Evaluate each strategy\n",
    "for strategy_name, performance in strategy_results.items():\n",
    "    print(f\"\\nüìä Evaluating {strategy_name}...\")\n",
    "    \n",
    "    # Calculate BLEU scores\n",
    "    bleu_scores = []\n",
    "    execution_accuracy = []\n",
    "    \n",
    "    for i, (generated_sql, question_data) in enumerate(zip(performance['generated_sqls'], training_questions)):\n",
    "        reference_sql = question_data['sql']\n",
    "        \n",
    "        # BLEU score calculation\n",
    "        if generated_sql and reference_sql:\n",
    "            bleu_score = metrics_calculator._calculate_single_bleu(generated_sql, reference_sql)\n",
    "            bleu_scores.append(bleu_score)\n",
    "            \n",
    "            # Mock execution correctness (syntax check)\n",
    "            is_correct = 'SELECT' in generated_sql.upper() and 'FROM' in generated_sql.upper()\n",
    "            execution_accuracy.append(is_correct)\n",
    "        else:\n",
    "            bleu_scores.append(0.0)\n",
    "            execution_accuracy.append(False)\n",
    "    \n",
    "    # Calculate strategy metrics\n",
    "    strategy_metrics = {\n",
    "        'success_rate': performance['success_rate'],\n",
    "        'avg_confidence': performance['avg_confidence'],\n",
    "        'avg_execution_time': performance['avg_execution_time'],\n",
    "        'avg_bleu_score': np.mean(bleu_scores),\n",
    "        'execution_accuracy': np.mean(execution_accuracy),\n",
    "        'total_questions': len(training_questions)\n",
    "    }\n",
    "    \n",
    "    evaluation_results['strategy_metrics'][strategy_name] = strategy_metrics\n",
    "    \n",
    "    print(f\"   ‚úÖ Success Rate: {strategy_metrics['success_rate']:.1%}\")\n",
    "    print(f\"   üéØ BLEU Score: {strategy_metrics['avg_bleu_score']:.3f}\")\n",
    "    print(f\"   ‚ö° Execution Accuracy: {strategy_metrics['execution_accuracy']:.1%}\")\n",
    "    print(f\"   üïí Avg Time: {strategy_metrics['avg_execution_time']:.4f}s\")\n",
    "\n",
    "# Overall performance summary\n",
    "all_success_rates = [m['success_rate'] for m in evaluation_results['strategy_metrics'].values()]\n",
    "all_bleu_scores = [m['avg_bleu_score'] for m in evaluation_results['strategy_metrics'].values()]\n",
    "all_execution_accuracy = [m['execution_accuracy'] for m in evaluation_results['strategy_metrics'].values()]\n",
    "\n",
    "evaluation_results['overall_performance'] = {\n",
    "    'best_strategy': max(evaluation_results['strategy_metrics'].items(), key=lambda x: x[1]['avg_bleu_score'])[0],\n",
    "    'avg_success_rate': np.mean(all_success_rates),\n",
    "    'avg_bleu_score': np.mean(all_bleu_scores),\n",
    "    'avg_execution_accuracy': np.mean(all_execution_accuracy),\n",
    "    'total_strategies': len(evaluation_results['strategy_metrics']),\n",
    "    'total_questions': len(training_questions)\n",
    "}\n",
    "\n",
    "print(f\"\\nüèÜ OVERALL PERFORMANCE SUMMARY:\")\n",
    "print(f\"   ü•á Best Strategy: {evaluation_results['overall_performance']['best_strategy']}\")\n",
    "print(f\"   üìä Avg Success Rate: {evaluation_results['overall_performance']['avg_success_rate']:.1%}\")\n",
    "print(f\"   üéØ Avg BLEU Score: {evaluation_results['overall_performance']['avg_bleu_score']:.3f}\")\n",
    "print(f\"   ‚ö° Avg Execution Accuracy: {evaluation_results['overall_performance']['avg_execution_accuracy']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Performance Visualizations\n",
    "print(\"üìà Creating Performance Visualizations...\")\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('ML Model Performance Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Strategy Success Rates\n",
    "strategies = list(evaluation_results['strategy_metrics'].keys())\n",
    "success_rates = [evaluation_results['strategy_metrics'][s]['success_rate'] for s in strategies]\n",
    "\n",
    "axes[0,0].bar(strategies, success_rates, color=['#e74c3c', '#f39c12', '#27ae60', '#9b59b6'])\n",
    "axes[0,0].set_title('Strategy Success Rates')\n",
    "axes[0,0].set_ylabel('Success Rate')\n",
    "axes[0,0].set_ylim(0, 1.1)\n",
    "for i, v in enumerate(success_rates):\n",
    "    axes[0,0].text(i, v + 0.02, f'{v:.1%}', ha='center', fontweight='bold')\n",
    "\n",
    "# 2. BLEU Scores Comparison\n",
    "bleu_scores = [evaluation_results['strategy_metrics'][s]['avg_bleu_score'] for s in strategies]\n",
    "\n",
    "axes[0,1].bar(strategies, bleu_scores, color=['#3498db', '#e67e22', '#2ecc71', '#8e44ad'])\n",
    "axes[0,1].set_title('Average BLEU Scores')\n",
    "axes[0,1].set_ylabel('BLEU Score')\n",
    "axes[0,1].set_ylim(0, 1.0)\n",
    "for i, v in enumerate(bleu_scores):\n",
    "    axes[0,1].text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# 3. Execution Times\n",
    "exec_times = [evaluation_results['strategy_metrics'][s]['avg_execution_time'] * 1000 for s in strategies]  # Convert to ms\n",
    "\n",
    "axes[1,0].bar(strategies, exec_times, color=['#1abc9c', '#f1c40f', '#e74c3c', '#9b59b6'])\n",
    "axes[1,0].set_title('Average Execution Times')\n",
    "axes[1,0].set_ylabel('Time (ms)')\n",
    "for i, v in enumerate(exec_times):\n",
    "    axes[1,0].text(i, v + 0.001, f'{v:.2f}ms', ha='center', fontweight='bold')\n",
    "\n",
    "# 4. Overall Performance Radar\n",
    "categories = ['Success Rate', 'BLEU Score', 'Execution Accuracy', 'Speed (inv)']\n",
    "best_strategy = evaluation_results['overall_performance']['best_strategy']\n",
    "best_metrics = evaluation_results['strategy_metrics'][best_strategy]\n",
    "\n",
    "values = [\n",
    "    best_metrics['success_rate'],\n",
    "    best_metrics['avg_bleu_score'],\n",
    "    best_metrics['execution_accuracy'],\n",
    "    1 - min(best_metrics['avg_execution_time'], 0.1)  # Inverted speed (higher is better)\n",
    "]\n",
    "\n",
    "axes[1,1].pie(values, labels=categories, autopct='%1.1f%%', startangle=90)\n",
    "axes[1,1].set_title(f'Best Strategy Performance\\n({best_strategy})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_performance_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualizations created and saved as 'model_performance_analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 4: Generate Pickle File for Frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Complete Results for Frontend\n",
    "print(\"üíæ Preparing results for frontend integration...\")\n",
    "\n",
    "# Compile all results\n",
    "frontend_results = {\n",
    "    'metadata': {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'version': '1.0.0',\n",
    "        'description': 'ML Model Performance Demo - Complete Results',\n",
    "        'training_duration': training_time + rag_training_time,\n",
    "        'total_questions': len(training_questions),\n",
    "        'total_strategies': len(available_strategies),\n",
    "        'schema_tables': len(ecommerce_schema['tables'])\n",
    "    },\n",
    "    \n",
    "    'model_performance': {\n",
    "        'strategy_results': evaluation_results['strategy_metrics'],\n",
    "        'overall_summary': evaluation_results['overall_performance'],\n",
    "        'best_strategy': evaluation_results['overall_performance']['best_strategy']\n",
    "    },\n",
    "    \n",
    "    'rag_performance': {\n",
    "        'avg_retrieval_time': rag_performance['avg_retrieval_time'],\n",
    "        'avg_relevance_score': rag_performance['avg_relevance_score'],\n",
    "        'avg_context_items': rag_performance['avg_context_items'],\n",
    "        'total_schema_cards': len(rag_pipeline.card_builder.cards)\n",
    "    },\n",
    "    \n",
    "    'training_data': {\n",
    "        'questions': training_questions,\n",
    "        'schema': ecommerce_schema,\n",
    "        'categories': training_df['category'].value_counts().to_dict(),\n",
    "        'complexity_distribution': training_df['complexity'].value_counts().to_dict()\n",
    "    },\n",
    "    \n",
    "    'detailed_results': {\n",
    "        'question_by_question': [],\n",
    "        'strategy_comparisons': strategy_results\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add question-by-question results\n",
    "for i, question_data in enumerate(training_questions):\n",
    "    question_result = {\n",
    "        'question': question_data['question'],\n",
    "        'reference_sql': question_data['sql'],\n",
    "        'category': question_data['category'],\n",
    "        'complexity': question_data['complexity'],\n",
    "        'strategy_results': {}\n",
    "    }\n",
    "    \n",
    "    # Add results from each strategy\n",
    "    for strategy_name, performance in strategy_results.items():\n",
    "        if i < len(performance['generated_sqls']):\n",
    "            question_result['strategy_results'][strategy_name] = {\n",
    "                'generated_sql': performance['generated_sqls'][i],\n",
    "                'confidence': performance['confidences'][i],\n",
    "                'execution_time': performance['execution_times'][i]\n",
    "            }\n",
    "    \n",
    "    frontend_results['detailed_results']['question_by_question'].append(question_result)\n",
    "\n",
    "print(f\"‚úÖ Results compiled: {len(frontend_results)} main sections\")\n",
    "print(f\"üìä Question results: {len(frontend_results['detailed_results']['question_by_question'])} items\")\n",
    "print(f\"üéØ Strategy results: {len(frontend_results['model_performance']['strategy_results'])} strategies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results as Pickle File\n",
    "print(\"üíæ Saving results as pickle file...\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path('model_outputs')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save as pickle (binary format for Python)\n",
    "pickle_file = output_dir / 'model_results.pkl'\n",
    "with open(pickle_file, 'wb') as f:\n",
    "    pickle.dump(frontend_results, f)\n",
    "\n",
    "# Also save as JSON for web frontend\n",
    "json_file = output_dir / 'model_results.json'\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(frontend_results, f, indent=2, default=str)\n",
    "\n",
    "# Save training metadata\n",
    "metadata_file = output_dir / 'training_metadata.json'\n",
    "training_metadata = {\n",
    "    'training_completed': datetime.now().isoformat(),\n",
    "    'model_files': {\n",
    "        'pickle_file': str(pickle_file),\n",
    "        'json_file': str(json_file),\n",
    "        'visualization': 'model_performance_analysis.png'\n",
    "    },\n",
    "    'performance_summary': {\n",
    "        'best_strategy': frontend_results['model_performance']['best_strategy'],\n",
    "        'avg_accuracy': frontend_results['model_performance']['overall_summary']['avg_execution_accuracy'],\n",
    "        'avg_bleu_score': frontend_results['model_performance']['overall_summary']['avg_bleu_score']\n",
    "    },\n",
    "    'ready_for_frontend': True\n",
    "}\n",
    "\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(training_metadata, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Pickle file saved: {pickle_file}\")\n",
    "print(f\"‚úÖ JSON file saved: {json_file}\")\n",
    "print(f\"‚úÖ Metadata saved: {metadata_file}\")\n",
    "print(f\"üìä File sizes:\")\n",
    "print(f\"   Pickle: {pickle_file.stat().st_size / 1024:.1f} KB\")\n",
    "print(f\"   JSON: {json_file.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 5: Training Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Training Summary\n",
    "print(\"üéâ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary = frontend_results['model_performance']['overall_summary']\n",
    "best_strategy = summary['best_strategy']\n",
    "best_metrics = frontend_results['model_performance']['strategy_results'][best_strategy]\n",
    "\n",
    "print(f\"üèÜ BEST PERFORMING STRATEGY: {best_strategy.upper()}\")\n",
    "print(f\"   üìä Success Rate: {best_metrics['success_rate']:.1%}\")\n",
    "print(f\"   üéØ BLEU Score: {best_metrics['avg_bleu_score']:.3f}\")\n",
    "print(f\"   ‚ö° Execution Accuracy: {best_metrics['execution_accuracy']:.1%}\")\n",
    "print(f\"   üïí Avg Response Time: {best_metrics['avg_execution_time']:.4f}s\")\n",
    "\n",
    "print(f\"\\nüìà OVERALL PERFORMANCE:\")\n",
    "print(f\"   üéØ Average Success Rate: {summary['avg_success_rate']:.1%}\")\n",
    "print(f\"   üìä Average BLEU Score: {summary['avg_bleu_score']:.3f}\")\n",
    "print(f\"   ‚ö° Average Execution Accuracy: {summary['avg_execution_accuracy']:.1%}\")\n",
    "\n",
    "print(f\"\\nüîç RAG PIPELINE PERFORMANCE:\")\n",
    "rag_perf = frontend_results['rag_performance']\n",
    "print(f\"   ‚ö° Avg Retrieval Time: {rag_perf['avg_retrieval_time']:.4f}s\")\n",
    "print(f\"   üéØ Avg Relevance Score: {rag_perf['avg_relevance_score']:.3f}\")\n",
    "print(f\"   üìã Schema Cards Indexed: {rag_perf['total_schema_cards']}\")\n",
    "\n",
    "print(f\"\\nüíæ OUTPUT FILES READY FOR FRONTEND:\")\n",
    "print(f\"   üì¶ Pickle File: model_outputs/model_results.pkl\")\n",
    "print(f\"   üåê JSON File: model_outputs/model_results.json\")\n",
    "print(f\"   üìä Visualization: model_performance_analysis.png\")\n",
    "print(f\"   üìã Metadata: model_outputs/training_metadata.json\")\n",
    "\n",
    "print(f\"\\nüöÄ NEXT STEPS:\")\n",
    "print(f\"   1. Load model_results.pkl in your frontend application\")\n",
    "print(f\"   2. Use the JSON file for web-based frontends\")\n",
    "print(f\"   3. Display performance metrics and visualizations\")\n",
    "print(f\"   4. Run inference using the trained strategies\")\n",
    "\n",
    "print(f\"\\n‚úÖ Training pipeline completed successfully!\")\n",
    "print(f\"üìÖ Total training time: {training_time + rag_training_time:.2f} seconds\")\n",
    "print(f\"üéØ Ready for production deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Frontend Integration Code\n",
    "\n",
    "Use this code to load the trained model results in your frontend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: How to load results in frontend\n",
    "print(\"üìã Frontend Integration Example:\")\n",
    "print(\"\"\"\n",
    "# Python Frontend (Flask/Django/Streamlit)\n",
    "import pickle\n",
    "with open('model_outputs/model_results.pkl', 'rb') as f:\n",
    "    model_results = pickle.load(f)\n",
    "\n",
    "# Access results\n",
    "best_strategy = model_results['model_performance']['best_strategy']\n",
    "accuracy = model_results['model_performance']['overall_summary']['avg_execution_accuracy']\n",
    "questions = model_results['detailed_results']['question_by_question']\n",
    "\n",
    "# JavaScript Frontend (React/Vue/Angular)\n",
    "fetch('model_outputs/model_results.json')\n",
    "  .then(response => response.json())\n",
    "  .then(data => {\n",
    "    const bestStrategy = data.model_performance.best_strategy;\n",
    "    const accuracy = data.model_performance.overall_summary.avg_execution_accuracy;\n",
    "    displayResults(data);\n",
    "  });\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüéØ The model is now trained and ready for frontend integration!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}