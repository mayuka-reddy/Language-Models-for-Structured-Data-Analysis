{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Metrics Demo\n",
    "\n",
    "This notebook demonstrates how to evaluate and compare different NL-to-SQL models using various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "from app.metrics import ModelEvaluator\n",
    "from app.inference import NL2SQLInference\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model evaluator\n",
    "evaluator = ModelEvaluator()\n",
    "print(\"âœ… Model evaluator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset\n",
    "test_data = evaluator.create_test_dataset()\n",
    "\n",
    "print(f\"ğŸ“Š Created test dataset with {len(test_data)} test cases:\")\n",
    "print()\n",
    "\n",
    "for i, test_case in enumerate(test_data, 1):\n",
    "    print(f\"{i}. Question: {test_case['question']}\")\n",
    "    print(f\"   Expected SQL: {test_case['expected_sql'][:60]}...\")\n",
    "    print(f\"   Schema: {test_case['schema'][:50]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dummy Models for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy models with different performance characteristics\n",
    "models = evaluator.create_dummy_models()\n",
    "\n",
    "print(f\"ğŸ¤– Created {len(models)} dummy models for comparison:\")\n",
    "for model_name, model in models.items():\n",
    "    print(f\"  - {model_name}: {model.name} (accuracy: {model.accuracy:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Individual Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test individual metric calculations\n",
    "print(\"ğŸ§ª Testing Individual Metrics:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test exact match\n",
    "sql1 = \"SELECT * FROM customers\"\n",
    "sql2 = \"SELECT * FROM customers\"\n",
    "sql3 = \"SELECT name FROM customers\"\n",
    "\n",
    "exact_match_1 = evaluator._check_exact_match(sql1, sql2)\n",
    "exact_match_2 = evaluator._check_exact_match(sql1, sql3)\n",
    "\n",
    "print(f\"Exact Match Test:\")\n",
    "print(f\"  '{sql1}' vs '{sql2}': {exact_match_1}\")\n",
    "print(f\"  '{sql1}' vs '{sql3}': {exact_match_2}\")\n",
    "print()\n",
    "\n",
    "# Test BLEU score\n",
    "bleu_score_1 = evaluator._calculate_bleu_score(sql1, sql2)\n",
    "bleu_score_2 = evaluator._calculate_bleu_score(sql1, sql3)\n",
    "\n",
    "print(f\"BLEU Score Test:\")\n",
    "print(f\"  '{sql1}' vs '{sql2}': {bleu_score_1:.3f}\")\n",
    "print(f\"  '{sql1}' vs '{sql3}': {bleu_score_2:.3f}\")\n",
    "print()\n",
    "\n",
    "# Test schema compliance\n",
    "schema = \"customers(customer_id, name, email, region)\"\n",
    "compliant_sql = \"SELECT * FROM customers\"\n",
    "non_compliant_sql = \"SELECT * FROM orders\"\n",
    "\n",
    "compliance_1 = evaluator._check_schema_compliance(compliant_sql, schema)\n",
    "compliance_2 = evaluator._check_schema_compliance(non_compliant_sql, schema)\n",
    "\n",
    "print(f\"Schema Compliance Test:\")\n",
    "print(f\"  '{compliant_sql}' with schema '{schema}': {compliance_1}\")\n",
    "print(f\"  '{non_compliant_sql}' with schema '{schema}': {compliance_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on all models\n",
    "print(\"ğŸ”„ Running model evaluation...\")\n",
    "results_df = evaluator.evaluate_models(test_data, models)\n",
    "\n",
    "print(\"âœ… Evaluation completed!\")\n",
    "print(\"\\nğŸ“Š Results:\")\n",
    "print(results_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Model Comparison Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Execution Accuracy\n",
    "axes[0, 0].bar(results_df['model'], results_df['execution_accuracy'])\n",
    "axes[0, 0].set_title('Execution Accuracy')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Exact Match Accuracy\n",
    "axes[0, 1].bar(results_df['model'], results_df['exact_match_accuracy'])\n",
    "axes[0, 1].set_title('Exact Match Accuracy')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# BLEU Score\n",
    "axes[1, 0].bar(results_df['model'], results_df['avg_bleu_score'])\n",
    "axes[1, 0].set_title('Average BLEU Score')\n",
    "axes[1, 0].set_ylabel('BLEU Score')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Response Time\n",
    "axes[1, 1].bar(results_df['model'], results_df['avg_response_time'])\n",
    "axes[1, 1].set_title('Average Response Time')\n",
    "axes[1, 1].set_ylabel('Time (seconds)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive performance heatmap\n",
    "metrics_for_heatmap = [\n",
    "    'execution_accuracy', 'exact_match_accuracy', \n",
    "    'schema_compliance_rate', 'avg_bleu_score'\n",
    "]\n",
    "\n",
    "heatmap_data = results_df.set_index('model')[metrics_for_heatmap].T\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_data, annot=True, cmap='YlOrRd', fmt='.3f', \n",
    "            cbar_kws={'label': 'Score'})\n",
    "plt.title('Model Performance Heatmap', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Metrics')\n",
    "plt.xlabel('Models')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Comparison Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed comparison report\n",
    "report = evaluator.generate_comparison_report(results_df)\n",
    "print(\"ğŸ“‹ Detailed Comparison Report:\")\n",
    "print(\"=\" * 50)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create overall ranking based on multiple metrics\n",
    "# Weight different metrics (you can adjust these weights)\n",
    "weights = {\n",
    "    'execution_accuracy': 0.4,\n",
    "    'exact_match_accuracy': 0.2,\n",
    "    'schema_compliance_rate': 0.2,\n",
    "    'avg_bleu_score': 0.2\n",
    "}\n",
    "\n",
    "# Calculate weighted score\n",
    "results_df['weighted_score'] = 0\n",
    "for metric, weight in weights.items():\n",
    "    results_df['weighted_score'] += results_df[metric] * weight\n",
    "\n",
    "# Sort by weighted score\n",
    "ranking_df = results_df.sort_values('weighted_score', ascending=False)\n",
    "\n",
    "print(\"ğŸ† Model Ranking (Weighted Score):\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for i, (_, row) in enumerate(ranking_df.iterrows(), 1):\n",
    "    print(f\"{i}. {row['model']}\")\n",
    "    print(f\"   Weighted Score: {row['weighted_score']:.3f}\")\n",
    "    print(f\"   Execution Accuracy: {row['execution_accuracy']:.1%}\")\n",
    "    print(f\"   Response Time: {row['avg_response_time']:.3f}s\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze error patterns\n",
    "print(\"ğŸ” Error Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "for _, row in results_df.iterrows():\n",
    "    model_name = row['model']\n",
    "    error_rate = row['error_rate']\n",
    "    \n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Error Rate: {error_rate:.1%}\")\n",
    "    print(f\"  Successful Queries: {row['total_queries'] - row['errors']}/{row['total_queries']}\")\n",
    "    \n",
    "    # Performance insights\n",
    "    if error_rate > 0.2:\n",
    "        print(f\"  âš ï¸  High error rate - needs improvement\")\n",
    "    elif error_rate > 0.1:\n",
    "        print(f\"  âš¡ Moderate error rate - room for optimization\")\n",
    "    else:\n",
    "        print(f\"  âœ… Low error rate - good performance\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance vs Accuracy Trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance vs accuracy trade-off\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "scatter = plt.scatter(results_df['avg_response_time'], \n",
    "                     results_df['execution_accuracy'],\n",
    "                     s=results_df['avg_bleu_score'] * 500,  # Size based on BLEU score\n",
    "                     alpha=0.7,\n",
    "                     c=range(len(results_df)),\n",
    "                     cmap='viridis')\n",
    "\n",
    "# Add model labels\n",
    "for i, row in results_df.iterrows():\n",
    "    plt.annotate(row['model'], \n",
    "                (row['avg_response_time'], row['execution_accuracy']),\n",
    "                xytext=(5, 5), textcoords='offset points',\n",
    "                fontsize=10, alpha=0.8)\n",
    "\n",
    "plt.xlabel('Average Response Time (seconds)')\n",
    "plt.ylabel('Execution Accuracy')\n",
    "plt.title('Performance vs Accuracy Trade-off\\n(Bubble size = BLEU Score)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add ideal region\n",
    "plt.axhline(y=0.8, color='green', linestyle='--', alpha=0.5, label='Good Accuracy (>80%)')\n",
    "plt.axvline(x=2.0, color='red', linestyle='--', alpha=0.5, label='Slow Response (>2s)')\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to CSV for further analysis\n",
    "output_file = 'model_evaluation_results.csv'\n",
    "results_df.to_csv(output_file, index=False)\n",
    "print(f\"ğŸ“ Results exported to {output_file}\")\n",
    "\n",
    "# Show summary statistics\n",
    "print(\"\\nğŸ“ˆ Summary Statistics:\")\n",
    "print(results_df[['execution_accuracy', 'exact_match_accuracy', \n",
    "                  'schema_compliance_rate', 'avg_bleu_score', \n",
    "                  'avg_response_time']].describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations based on results\n",
    "print(\"ğŸ’¡ Recommendations:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "best_model = results_df.loc[results_df['execution_accuracy'].idxmax()]\n",
    "fastest_model = results_df.loc[results_df['avg_response_time'].idxmin()]\n",
    "most_consistent = results_df.loc[results_df['schema_compliance_rate'].idxmax()]\n",
    "\n",
    "print(f\"ğŸ† Best Overall Accuracy: {best_model['model']} ({best_model['execution_accuracy']:.1%})\")\n",
    "print(f\"âš¡ Fastest Response: {fastest_model['model']} ({fastest_model['avg_response_time']:.3f}s)\")\n",
    "print(f\"ğŸ¯ Most Schema Compliant: {most_consistent['model']} ({most_consistent['schema_compliance_rate']:.1%})\")\n",
    "\n",
    "print(\"\\nğŸ“‹ Action Items:\")\n",
    "if results_df['execution_accuracy'].max() < 0.9:\n",
    "    print(\"â€¢ Consider fine-tuning models on domain-specific data\")\n",
    "if results_df['avg_response_time'].max() > 3.0:\n",
    "    print(\"â€¢ Optimize inference pipeline for better response times\")\n",
    "if results_df['schema_compliance_rate'].min() < 0.8:\n",
    "    print(\"â€¢ Improve schema awareness in model training\")\n",
    "\n",
    "print(\"\\nâœ… Evaluation demo completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}