"""
Metrics module for backward compatibility.
Wraps the enhanced metrics calculator from models/evaluation.
"""

import sys
from pathlib import Path

# Add models directory to path
models_path = Path(__file__).parent.parent / "models"
sys.path.insert(0, str(models_path))

try:
    from evaluation.metrics_calculator import MetricsCalculator, EvaluationResult
    from evaluation.comparator import ModelComparator
    from evaluation.visualizer import PerformanceVisualizer
except ImportError:
    # Fallback for tests
    MetricsCalculator = None
    EvaluationResult = None
    ModelComparator = None
    PerformanceVisualizer = None


class ModelEvaluator:
    """
    Backward compatibility wrapper for the enhanced metrics calculator.
    """
    
    def __init__(self, db_path=None):
        """Initialize model evaluator."""
        if MetricsCalculator:
            self.metrics_calculator = MetricsCalculator(db_path)
            self.comparator = ModelComparator(self.metrics_calculator)
            self.visualizer = PerformanceVisualizer()
        else:
            self.metrics_calculator = None
            self.comparator = None
            self.visualizer = None
        
        # For backward compatibility with tests
        self.smoothing_function = getattr(self.metrics_calculator, 'smoothing_function', None)
    
    def _check_exact_match(self, sql1, sql2):
        """Check exact match between SQL queries."""
        if self.metrics_calculator:
            return self.metrics_calculator._check_exact_match(sql1, sql2)
        return sql1.strip().upper() == sql2.strip().upper()
    
    def _normalize_sql(self, sql):
        """Normalize SQL query."""
        if self.metrics_calculator:
            return self.metrics_calculator._normalize_sql(sql)
        return sql.strip().upper()
    
    def _tokenize_sql(self, sql):
        """Tokenize SQL query."""
        if self.metrics_calculator:
            return self.metrics_calculator._tokenize_sql(sql)
        return sql.lower().split()
    
    def _calculate_bleu_score(self, predicted, expected):
        """Calculate BLEU score."""
        if self.metrics_calculator:
            return self.metrics_calculator._calculate_single_bleu(predicted, expected)
        return 1.0 if predicted.strip().upper() == expected.strip().upper() else 0.0
    
    def _check_schema_compliance(self, sql, schema):
        """Check schema compliance."""
        if self.metrics_calculator:
            schema_info = {'tables': {}} if isinstance(schema, str) else schema
            return self.metrics_calculator._check_schema_compliance(sql, schema_info)
        return True  # Default to compliant for backward compatibility
    
    def _check_execution_correctness(self, predicted, expected):
        """Check execution correctness."""
        if self.metrics_calculator:
            return self.metrics_calculator.calculate_execution_correctness(predicted, expected)
        return self._check_exact_match(predicted, expected)
    
    def create_dummy_models(self):
        """Create dummy models for testing."""
        class DummyModel:
            def __init__(self, name):
                self.name = name
            
            def generate_sql(self, question, schema=""):
                return {
                    'sql': f'SELECT * FROM table -- Generated by {self.name}',
                    'confidence': 0.8
                }
        
        return {
            'dummy_model_1': DummyModel('Model1'),
            'dummy_model_2': DummyModel('Model2')
        }
    
    def create_test_dataset(self):
        """Create test dataset."""
        return [
            {
                'question': 'Show me all customers',
                'expected_sql': 'SELECT * FROM customers',
                'schema': 'customers(id, name, email)'
            },
            {
                'question': 'Count total orders',
                'expected_sql': 'SELECT COUNT(*) FROM orders',
                'schema': 'orders(id, customer_id, amount)'
            }
        ]
    
    def _evaluate_single_case(self, test_case, model):
        """Evaluate single test case."""
        try:
            result = model.generate_sql(test_case['question'], test_case.get('schema', ''))
            predicted_sql = result.get('sql', '')
            expected_sql = test_case['expected_sql']
            
            return {
                'execution_correct': self._check_execution_correctness(predicted_sql, expected_sql),
                'exact_match': self._check_exact_match(predicted_sql, expected_sql),
                'schema_compliant': self._check_schema_compliance(predicted_sql, test_case.get('schema', '')),
                'bleu_score': self._calculate_bleu_score(predicted_sql, expected_sql),
                'response_time': 0.1,  # Mock response time
                'error': False
            }
        except Exception:
            return {
                'execution_correct': False,
                'exact_match': False,
                'schema_compliant': False,
                'bleu_score': 0.0,
                'response_time': 0.0,
                'error': True
            }
    
    def evaluate_models(self, test_data, models):
        """Evaluate multiple models."""
        import pandas as pd
        
        results = []
        
        for model_name, model in models.items():
            model_results = []
            
            for test_case in test_data:
                case_result = self._evaluate_single_case(test_case, model)
                model_results.append(case_result)
            
            # Aggregate results
            total_queries = len(model_results)
            execution_accuracy = sum(r['execution_correct'] for r in model_results) / total_queries
            exact_match_accuracy = sum(r['exact_match'] for r in model_results) / total_queries
            schema_compliance_rate = sum(r['schema_compliant'] for r in model_results) / total_queries
            avg_bleu_score = sum(r['bleu_score'] for r in model_results) / total_queries
            avg_response_time = sum(r['response_time'] for r in model_results) / total_queries
            error_rate = sum(r['error'] for r in model_results) / total_queries
            
            results.append({
                'model': model_name,
                'total_queries': total_queries,
                'execution_accuracy': execution_accuracy,
                'exact_match_accuracy': exact_match_accuracy,
                'schema_compliance_rate': schema_compliance_rate,
                'avg_bleu_score': avg_bleu_score,
                'avg_response_time': avg_response_time,
                'error_rate': error_rate
            })
        
        return pd.DataFrame(results)
    
    def generate_comparison_report(self, results_df):
        """Generate comparison report."""
        report_lines = [
            "Model Comparison Report",
            "=" * 50,
            ""
        ]
        
        for _, row in results_df.iterrows():
            report_lines.extend([
                f"Model: {row['model']}",
                f"  Execution Accuracy: {row['execution_accuracy']:.1%}",
                f"  Exact Match: {row['exact_match_accuracy']:.1%}",
                f"  Schema Compliance: {row['schema_compliance_rate']:.1%}",
                f"  BLEU Score: {row['avg_bleu_score']:.3f}",
                f"  Response Time: {row['avg_response_time']:.3f}s",
                f"  Error Rate: {row['error_rate']:.1%}",
                ""
            ])
        
        return "\n".join(report_lines)