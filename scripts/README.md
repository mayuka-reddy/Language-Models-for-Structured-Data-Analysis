# Training Scripts

## Quick Start

### Run Complete Training Pipeline

```bash
# From project root
python scripts/train_all_techniques.py
```

This will:
1. âœ… Train all 5 prompting techniques (Zero-Shot, Few-Shot, CoT, Self-Consistency, Least-to-Most)
2. âœ… Evaluate RAG pipeline (target: 0.75+ relevance)
3. âœ… Generate comprehensive metrics tables (CSV + JSON)
4. âœ… Create visualizations (PNG + interactive HTML)
5. âœ… Generate summary reports (Markdown)
6. âœ… Save model artifacts (PKL + JSON)

### Output Structure

```
results/
â”œâ”€â”€ metrics/
â”‚   â”œâ”€â”€ overall_performance.csv      # Performance comparison table
â”‚   â”œâ”€â”€ overall_performance.json
â”‚   â”œâ”€â”€ rag_performance.csv          # RAG evaluation metrics
â”‚   â””â”€â”€ rag_performance.json
â”œâ”€â”€ visualizations/
â”‚   â”œâ”€â”€ strategy_comparison.png      # Bar charts comparing strategies
â”‚   â”œâ”€â”€ rag_relevance_distribution.png  # RAG relevance histogram
â”‚   â””â”€â”€ interactive_comparison.html  # Interactive Plotly chart
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ trained_results.pkl          # Complete results (Python)
â”‚   â””â”€â”€ trained_results.json         # Complete results (JSON)
â””â”€â”€ reports/
    â””â”€â”€ training_summary.md          # Comprehensive summary report
```

### Command Line Options

```bash
# Custom output directory
python scripts/train_all_techniques.py --output my_results

# Verbose output
python scripts/train_all_techniques.py --verbose

# Help
python scripts/train_all_techniques.py --help
```

### Expected Output

```
ğŸš€ STARTING COMPLETE NL-TO-SQL TRAINING PIPELINE
==================================================================

ğŸ”§ TRAINING ALL PROMPTING STRATEGIES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“ Training: ZERO SHOT
   âœ“ Processed 3/10 questions
   âœ“ Processed 6/10 questions
   âœ“ Processed 9/10 questions
   
   ğŸ“Š Results:
      âœ… Success Rate: 100.0%
      ğŸ¯ Avg BLEU Score: 0.720
      ğŸ“‹ Schema Compliance: 100.0%
      ğŸ² Avg Confidence: 0.752
      âš¡ Avg Time: 0.0012s

[... continues for all 5 strategies ...]

ğŸ” EVALUATING RAG PIPELINE
==================================================================
ğŸ“Š RAG Performance:
   âš¡ Avg Retrieval Time: 0.0234s
   ğŸ¯ Avg Relevance Score: 0.762
   ğŸ“ˆ Min/Max Relevance: 0.680 / 0.850
   ğŸ“‹ Schema Cards: 45
   âœ… Target (0.75+): ACHIEVED

ğŸ“Š Generating Metrics Tables...
   âœ… Saved: overall_performance.csv/json
   âœ… Saved: rag_performance.csv/json

ğŸ“ˆ Generating Visualizations...
   âœ… Saved: strategy_comparison.png
   âœ… Saved: rag_relevance_distribution.png
   âœ… Saved: interactive_comparison.html

ğŸ“ Generating Reports...
   âœ… Saved: training_summary.md

ğŸ’¾ Saving Model Artifacts...
   âœ… Saved: trained_results.pkl
   âœ… Saved: trained_results.json

ğŸ‰ TRAINING PIPELINE COMPLETE!
==================================================================
â±ï¸  Total Time: 12.45 seconds
ğŸ“ Results saved to: /path/to/results

ğŸ“Š Summary:
   âœ… Trained 5 strategies
   âœ… RAG relevance: 0.762 (Target: 0.75+)
   âœ… Generated visualizations and reports
   âœ… Created comprehensive metrics tables

ğŸ¯ Next Steps:
   1. Review results in: results/reports/training_summary.md
   2. View visualizations in: results/visualizations/
   3. Check metrics in: results/metrics/
   4. Load model artifacts from: results/models/trained_results.pkl

âœ¨ Training pipeline ready for presentation!
```

### Loading Results in Python

```python
import pickle
import pandas as pd

# Load complete results
with open('results/models/trained_results.pkl', 'rb') as f:
    results = pickle.load(f)

# Access strategy performance
for strategy, metrics in results['strategy_results'].items():
    print(f"{strategy}: BLEU={metrics['avg_bleu_score']:.3f}")

# Load metrics as DataFrame
perf_df = pd.read_csv('results/metrics/overall_performance.csv')
print(perf_df)

# Check RAG performance
rag_df = pd.read_csv('results/metrics/rag_performance.csv')
print(f"RAG Relevance: {rag_df['avg_relevance_score'].values[0]:.3f}")
```

### Integration with Frontend

```python
# Use results in frontend_integration.py
from pathlib import Path
import pickle

results_file = Path('results/models/trained_results.pkl')
with open(results_file, 'rb') as f:
    model_results = pickle.load(f)

# Now use model_results in your Gradio UI
```

## Troubleshooting

### Import Errors
```bash
# Ensure you're in project root
cd /path/to/Language-Models-for-Structured-Data-Analysis

# Install dependencies
pip install -r requirements.txt
```

### Permission Errors
```bash
# Make script executable
chmod +x scripts/train_all_techniques.py
```

### Memory Issues
The script uses mock responses by default. For actual LLM inference, ensure sufficient RAM (8GB+ recommended).

## Customization

### Adding More Training Questions

Edit the `load_training_data()` method in the script:

```python
def load_training_data(self):
    return [
        {
            'question': 'Your new question',
            'sql': 'SELECT ...',
            'category': 'your_category',
            'complexity': 'simple|medium|complex',
            'requires_join': True|False
        },
        # ... more questions
    ]
```

### Changing Output Format

Modify the `generate_metrics_tables()` or `generate_visualizations()` methods to customize output formats.

### Using Real LLM

Replace the `generate_mock_response()` method with actual LLM API calls:

```python
def generate_mock_response(self, question, strategy_name, reference_sql, confidence_base):
    # Replace with actual LLM call
    response = your_llm_api.generate(prompt)
    return response
```

## Performance Benchmarks

Expected execution times (on 8-core CPU, 16GB RAM):
- Training all strategies: ~5-10 seconds
- RAG evaluation: ~2-3 seconds
- Visualization generation: ~3-5 seconds
- Total pipeline: ~12-20 seconds

## Next Steps

After running the script:
1. Review `results/reports/training_summary.md` for comprehensive analysis
2. Open `results/visualizations/interactive_comparison.html` in browser
3. Use `results/models/trained_results.pkl` for frontend integration
4. Share `results/metrics/*.csv` for presentations

## Support

For issues or questions, refer to:
- Main README: `../README.md`
- Improvements Summary: `../IMPROVEMENTS_SUMMARY.md`
- Project Structure: `../PROJECT_STRUCTURE.md`